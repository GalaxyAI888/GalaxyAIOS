import asyncio
from concurrent.futures import ProcessPoolExecutor
from functools import partial
import glob
from itertools import chain
import logging
import os
import re
import time
from typing import Dict, Tuple
from multiprocessing import Manager, cpu_count


from aistack.config.config import Config
from aistack.log import setup_logging
from aistack.schemas.model_files import ModelFile, ModelFileUpdate, ModelFileStateEnum
from aistack.schemas.models import SourceEnum
from aistack.client import ClientSet
from aistack.server.bus import Event, EventType
from aistack.utils.file import delete_path
from aistack.worker import downloaders


logger = logging.getLogger(__name__)

max_concurrent_downloads = 5


class ModelFileManager:
    def __init__(
        self,
        worker_id: int,
        clientset: ClientSet,
        cfg: Config,
    ):
        self._worker_id = worker_id
        self._config = cfg
        self._clientset = clientset
        self._active_downloads: Dict[int, Tuple] = {}
        self._download_pool = None

    async def watch_model_files(self):
        self._prerun()
        while True:
            try:
                logger.debug("Started watching model files.")
                await self._clientset.model_files.awatch(
                    callback=self._handle_model_file_event
                )
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Failed to watch model files: {e}")
                await asyncio.sleep(5)

    def _prerun(self):
        self._mp_manager = Manager()
        self._download_pool = ProcessPoolExecutor(
            max_workers=min(max_concurrent_downloads, cpu_count()),
        )

    def _handle_model_file_event(self, event: Event):
        mf = ModelFile.model_validate(event.data)

        if mf.worker_id != self._worker_id:
            # Ignore model files that are not assigned to this worker.
            return

        logger.debug(f"Received model file event: {event.type} {mf.id} {mf.state}")

        if event.type == EventType.DELETED:
            asyncio.create_task(self._handle_deletion(mf))
        elif event.type in {EventType.CREATED, EventType.UPDATED}:
            if mf.state != ModelFileStateEnum.DOWNLOADING:
                return
            self._create_download_task(mf)

    def _update_model_file(self, id: int, **kwargs):
        model_file_public = self._clientset.model_files.get(id=id)

        model_file_update = ModelFileUpdate(**model_file_public.model_dump())
        for key, value in kwargs.items():
            setattr(model_file_update, key, value)

        self._clientset.model_files.update(id=id, model_update=model_file_update)

    
    async def _handle_deletion(self, model_file: ModelFile):
        entry = self._active_downloads.pop(model_file.id, None)
        if entry:
            future, cancel_flag = entry
            logger.info(
                f"Requesting cancellation for deleted model: {model_file.readable_source}(id: {model_file.id})"
            )
            cancel_flag.set()
            
            # å°è¯•å–æ¶ˆ futureï¼ˆåªå¯¹å°šæœªå¼€å§‹çš„ä»»åŠ¡æœ‰æ•ˆï¼‰
            if future.cancel():
                logger.info(f"Download task cancelled before starting: {model_file.readable_source}")
            else:
                logger.info(f"Download task already running, waiting for completion: {model_file.readable_source}")
                # å¯¹äºŽå·²ç»åœ¨è¿è¡Œçš„ä¸‹è½½ï¼Œæˆ‘ä»¬éœ€è¦ç­‰å¾…å®ƒå®Œæˆæˆ–ä¸­æ–­
                try:
                    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´çœ‹æ˜¯å¦ä¼šè¢«æ£€æŸ¥ç‚¹ä¸­æ–­
                    await asyncio.wait_for(asyncio.wrap_future(future), timeout=5)
                except asyncio.TimeoutError:
                    logger.warning(f"Download still running after 5s, may need manual cleanup: {model_file.readable_source}")
                except Exception as e:
                    logger.debug(f"Future completed with error: {e}")
            
            # æ¸…ç†å·²ä¸‹è½½çš„æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            if model_file.local_dir or model_file.resolved_paths:
                await self._cleanup_orphaned_files(model_file)

        if model_file.cleanup_on_delete:
            await self._delete_model_file(model_file)
    
    async def _cleanup_orphaned_files(self, model_file: ModelFile):
        """æ¸…ç†å› å–æ¶ˆä¸‹è½½è€Œæ®‹ç•™çš„æ–‡ä»¶"""
        try:
            # æ¸…ç†æœ¬åœ°ç›®å½•
            if model_file.local_dir and os.path.exists(model_file.local_dir):
                import shutil
                shutil.rmtree(model_file.local_dir, ignore_errors=True)
                logger.info(f"Cleaned up local directory: {model_file.local_dir}")
            
            # æ¸…ç†ç¼“å­˜ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶
            if hasattr(model_file, 'source') and model_file.source == SourceEnum.OLLAMA_LIBRARY.value:
                if hasattr(model_file, 'ollama_library_model_name') and model_file.ollama_library_model_name:
                    sanitized_name = re.sub(r"[^a-zA-Z0-9]", "_", model_file.ollama_library_model_name)
                    model_path = os.path.join(self._config.cache_dir, "ollama", sanitized_name)
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    temp_file = model_path + ".part"
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        logger.info(f"Cleaned up temp file: {temp_file}")
                    # æ¸…ç†æ¨¡åž‹æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                    if os.path.exists(model_path):
                        os.remove(model_path)
                        logger.info(f"Cleaned up model file: {model_path}")
            
        except Exception as e:
            logger.warning(f"Failed to cleanup orphaned files: {e}")

    async def _delete_model_file(self, model_file: ModelFile):
        try:
            if model_file.resolved_paths:
                paths = chain.from_iterable(
                    glob.glob(p) if '*' in p else [p] for p in model_file.resolved_paths
                )
                for path in paths:
                    delete_path(path)

            logger.info(
                f"Deleted model file {model_file.readable_source}(id: {model_file.id})"
            )
        except Exception as e:
            logger.error(
                f"Failed to delete {model_file.readable_source}(id: {model_file.id}: {e}"
            )
            await self._update_model_file(
                model_file.id,
                state=ModelFileStateEnum.ERROR,
                state_message=f"Deletion failed: {str(e)}",
            )

    def _create_download_task(self, model_file: ModelFile):
        if model_file.id in self._active_downloads:
            return

        cancel_flag = self._mp_manager.Event()

        download_task = ModelFileDownloadTask(model_file, self._config, cancel_flag)
        future = self._download_pool.submit(download_task.run)
        self._active_downloads[model_file.id] = (future, cancel_flag)

        logger.debug(f"Created download task for {model_file.readable_source}")

        async def _check_completion():
            try:
                await asyncio.wrap_future(future)
                logger.info(f"ðŸŽ‰ Download task completed successfully: {model_file.readable_source}")
            except asyncio.CancelledError:
                logger.info(f"â¹ï¸ Download task cancelled: {model_file.readable_source}")
            except Exception as e:
                logger.error(f"ðŸ’¥ Download task failed: {model_file.readable_source} - {e}")
                await self._update_model_file(
                    model_file.id,
                    state=ModelFileStateEnum.ERROR,
                    state_message=str(e),
                )
            finally:
                self._active_downloads.pop(model_file.id, None)
                logger.debug(f"Download task finished for {model_file.readable_source}")

        asyncio.create_task(_check_completion())


class ModelFileDownloadTask:

    def __init__(self, model_file: ModelFile, cfg: Config, cancel_flag):
        self._model_file = model_file
        self._config = cfg
        self._cancel_flag = cancel_flag
    def _get_existing_file_size(self) -> int:
        """èŽ·å–å·²ä¸‹è½½æ–‡ä»¶çš„å®žé™…å¤§å°"""
        try:
            total_size = 0
            
            # æ£€æŸ¥æœ¬åœ°ç›®å½•ä¸­çš„æ–‡ä»¶
            if self._model_file.local_dir and os.path.exists(self._model_file.local_dir):
                # å¦‚æžœæ˜¯æ•´ä¸ªç›®å½•ä¸‹è½½ï¼Œè®¡ç®—ç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶çš„å¤§å°
                if not self._model_file.model_scope_file_path and not self._model_file.huggingface_filename:
                    # æ•´ä¸ªç›®å½•ä¸‹è½½
                    for root, dirs, files in os.walk(self._model_file.local_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            if os.path.exists(file_path):
                                try:
                                    total_size += os.path.getsize(file_path)
                                except (OSError, IOError) as e:
                                    logger.warning(f"Failed to get size of {file_path}: {e}")
                                    continue
                else:
                    # å•ä¸ªæ–‡ä»¶ä¸‹è½½ï¼Œæ£€æŸ¥ç‰¹å®šæ–‡ä»¶
                    if self._model_file.model_scope_file_path:
                        # ModelScope å•ä¸ªæ–‡ä»¶
                        file_path = os.path.join(self._model_file.local_dir, self._model_file.model_scope_file_path)
                        if os.path.exists(file_path):
                            try:
                                total_size += os.path.getsize(file_path)
                            except (OSError, IOError) as e:
                                logger.warning(f"Failed to get size of {file_path}: {e}")
                    elif self._model_file.huggingface_filename:
                        # HuggingFace å•ä¸ªæ–‡ä»¶
                        file_path = os.path.join(self._model_file.local_dir, self._model_file.huggingface_filename)
                        if os.path.exists(file_path):
                            try:
                                total_size += os.path.getsize(file_path)
                            except (OSError, IOError) as e:
                                logger.warning(f"Failed to get size of {file_path}: {e}")
            
            # æ£€æŸ¥å·²è§£æžçš„æ–‡ä»¶è·¯å¾„
            if self._model_file.resolved_paths:
                for path in self._model_file.resolved_paths:
                    if os.path.exists(path):
                        if os.path.isfile(path):
                            try:
                                total_size += os.path.getsize(path)
                            except (OSError, IOError) as e:
                                logger.warning(f"Failed to get size of {path}: {e}")
                        elif os.path.isdir(path):
                            # å¦‚æžœæ˜¯ç›®å½•ï¼Œè®¡ç®—ç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶çš„å¤§å°
                            for root, dirs, files in os.walk(path):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    if os.path.exists(file_path):
                                        try:
                                            total_size += os.path.getsize(file_path)
                                        except (OSError, IOError) as e:
                                            logger.warning(f"Failed to get size of {file_path}: {e}")
                                            continue
            
            logger.debug(f"Existing file size for {self._model_file.readable_source}: {total_size} bytes")
            return total_size
            
        except Exception as e:
            logger.warning(f"Failed to calculate existing file size: {e}")
            return 0

    def prerun(self):
        setup_logging(self._config.debug)
        self._clientset = ClientSet(
            base_url=self._config.server_url,
            username=f"system/worker/{self._config.worker_ip}",
            password=self._config.token,
        )

        self._ensure_model_file_size()

        self._last_download_update_time = 0
        
        # æ£€æµ‹å·²ä¸‹è½½æ–‡ä»¶çš„å®žé™…å¤§å°ï¼Œé¿å…è¿›åº¦è®¡ç®—é”™è¯¯
        self._initial_downloaded_size = self._get_existing_file_size()
        
        logger.debug(f"Initializing task for {self._model_file.readable_source}")
        logger.debug(f"Existing downloaded size: {self._initial_downloaded_size} bytes")
        
        self._update_progress_func = partial(
            self._update_model_file_progress, self._model_file.id
        )
        self._model_file_size = self._model_file.size
        
        # å¦‚æžœå·²æœ‰éƒ¨åˆ†æ–‡ä»¶ï¼Œæ›´æ–°åˆå§‹è¿›åº¦
        if self._initial_downloaded_size > 0 and self._model_file_size > 0:
            initial_progress = round((self._initial_downloaded_size / self._model_file_size) * 100, 2)
            logger.info(f"Resuming download from {initial_progress}% ({self._initial_downloaded_size}/{self._model_file_size} bytes)")
            self._update_progress_func(initial_progress)
        
        self.hijack_tqdm_progress()


    def run(self):
        try:
            self.prerun()
            self._download_model_file()
        except asyncio.CancelledError:
            logger.info(f"Download cancelled for {self._model_file.readable_source}")
        except Exception as e:
            logger.error(
                f"Download failed for {self._model_file.readable_source}: {str(e)}"
            )
            self._update_model_file(
                self._model_file.id,
                state=ModelFileStateEnum.ERROR,
                state_message=str(e),
            )

    def _download_model_file(self):
        logger.info(f"Starting download of model file: {self._model_file.readable_source}")
        try:
            # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if self._cancel_flag.is_set():
                logger.info(f"Download cancelled before starting: {self._model_file.readable_source}")
                raise Exception("Download cancelled")
            
            model_paths = downloaders.download_model(
                self._model_file,
                local_dir=self._model_file.local_dir,
                cache_dir=self._config.cache_dir,
                ollama_library_base_url=self._config.ollama_library_base_url,
                huggingface_token=self._config.huggingface_token,
                cancel_flag=self._cancel_flag,
            )
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆï¼ˆä¸‹è½½æœŸé—´å¯èƒ½è¢«å–æ¶ˆï¼‰
            if self._cancel_flag.is_set():
                logger.info(f"Download cancelled after completion: {self._model_file.readable_source}")
                raise Exception("Download cancelled")
            
            # æ›´æ–°æ¨¡åž‹æ–‡ä»¶çŠ¶æ€ä¸ºå®Œæˆ
            self._update_model_file(
                self._model_file.id,
                state=ModelFileStateEnum.READY,
                download_progress=100,
                resolved_paths=model_paths,
            )
            
            logger.info(f"âœ… Download completed successfully: {self._model_file.readable_source}")
            logger.info(f"ðŸ“ Downloaded files: {model_paths}")
            
        except Exception as e:
            # å¦‚æžœè¢«å–æ¶ˆï¼Œåˆ é™¤å·²ä¸‹è½½çš„æ–‡ä»¶
            if self._cancel_flag.is_set():
                logger.info(f"Cleaning up cancelled download files for: {self._model_file.readable_source}")
                self._cleanup_downloaded_files()
            logger.error(f"âŒ Download failed for {self._model_file.readable_source}: {e}")
            raise
    
    def _cleanup_downloaded_files(self):
        """æ¸…ç†å·²ä¸‹è½½çš„æ–‡ä»¶"""
        try:
            # æ¸…ç†æœ¬åœ°ç›®å½•
            if self._model_file.local_dir and os.path.exists(self._model_file.local_dir):
                import shutil
                shutil.rmtree(self._model_file.local_dir, ignore_errors=True)
                logger.info(f"Cleaned up local directory: {self._model_file.local_dir}")
            
            # æ¸…ç†ç¼“å­˜ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶
            cache_dir = self._config.cache_dir
            if self._model_file.source == "ollama_library" and self._model_file.ollama_library_model_name:
                sanitized_name = re.sub(r"[^a-zA-Z0-9]", "_", self._model_file.ollama_library_model_name)
                model_path = os.path.join(cache_dir, "ollama", sanitized_name)
                temp_file = model_path + ".part"
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    logger.info(f"Cleaned up temp file: {temp_file}")
            
        except Exception as e:
            logger.warning(f"Failed to cleanup files: {e}")

    def hijack_tqdm_progress(task_self):
        """
        Monkey patch the tqdm progress bar to update the model instance download progress.
        tqdm is used by hf_hub_download under the hood.
        """
        from tqdm import tqdm

        _original_init = (
            tqdm._original_init if hasattr(tqdm, "_original_init") else tqdm.__init__
        )
        _original_update = (
            tqdm._original_update if hasattr(tqdm, "_original_update") else tqdm.update
        )

        def _new_init(self: tqdm, *args, **kwargs):
            kwargs["disable"] = False  # enable the progress bar anyway
            _original_init(self, *args, **kwargs)

            if hasattr(task_self, '_model_file_size'):
                # ä¸è¦åœ¨è¿™é‡Œç´¯ç§¯ï¼Œå› ä¸ºè¿™ä¼šé‡å¤è®¡ç®—å·²ä¸‹è½½çš„å¤§å°
                # åˆå§‹è¿›åº¦å·²ç»åœ¨prerunä¸­æ­£ç¡®è®¾ç½®äº†
                pass

        def _new_update(self: tqdm, n=1):
            _original_update(self, n)

            if task_self._cancel_flag.is_set():
                raise asyncio.CancelledError("Download cancelled")

            # This is the default for single tqdm downloader like ollama
            # TODO we may want to unify to always get the size before downloading.
            total_size = self.total
            downloaded_size = self.n
            
            if hasattr(task_self, '_model_file_size'):
                # This is summary for group downloading
                total_size = task_self._model_file_size
                
                # å¯¹äºŽæ•´ä¸ªç›®å½•ä¸‹è½½ï¼Œä½¿ç”¨tqdmçš„å½“å‰è¿›åº¦åŠ ä¸Šåˆå§‹å·²ä¸‹è½½å¤§å°
                # è¿™æ ·å¯ä»¥æ­£ç¡®åæ˜ æ–­ç‚¹ç»­ä¼ çš„è¿›åº¦
                initial_downloaded_size = getattr(task_self, '_initial_downloaded_size', 0)
                downloaded_size = initial_downloaded_size + self.n
                
                # æ·»åŠ è°ƒè¯•æ—¥å¿—
                logger.debug(f"Progress update: initial={initial_downloaded_size}, tqdm_n={self.n}, total={total_size}, calculated={downloaded_size}")

            try:
                # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”ï¼Œç¡®ä¿ä¸è¶…è¿‡100%
                progress_percent = round((downloaded_size / total_size) * 100, 2)
                
                # é™åˆ¶è¿›åº¦ä¸è¶…è¿‡100%ï¼Œé¿å…æ˜¾ç¤ºè¶…è¿‡100%çš„è¿›åº¦
                if progress_percent > 100:
                    progress_percent = 100.0
                    logger.warning(
                        f"Download progress exceeded 100% for {task_self._model_file.readable_source}: "
                        f"downloaded={downloaded_size}, total={total_size}, progress={progress_percent}%"
                    )

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è¿›åº¦
                should_update = (
                    # ä¸‹è½½å®Œæˆæ—¶ç«‹å³æ›´æ–°
                    downloaded_size >= total_size or
                    # æˆ–è€…è·ç¦»ä¸Šæ¬¡æ›´æ–°è¶…è¿‡2ç§’
                    time.time() - task_self._last_download_update_time >= 2
                )
                
                if should_update:
                    task_self._update_progress_func(progress_percent)
                    task_self._last_download_update_time = time.time()
                    
                    # å¦‚æžœä¸‹è½½å®Œæˆï¼Œè®°å½•æˆåŠŸæ—¥å¿—
                    if downloaded_size >= total_size and progress_percent >= 100:
                        logger.info(f"Download progress reached 100% for {task_self._model_file.readable_source}")
                        
            except Exception as e:
                logger.warning(f"Failed to update progress: {e}")

        tqdm.__init__ = _new_init
        tqdm.update = _new_update
        tqdm._original_init = _original_init
        tqdm._original_update = _original_update

    def _ensure_model_file_size(self):
        if self._model_file.size is not None:
            return

        size = downloaders.get_model_file_size(
            self._model_file,
            huggingface_token=self._config.huggingface_token,
            cache_dir=self._config.cache_dir,
            ollama_library_base_url=self._config.ollama_library_base_url,
        )
        self._model_file.size = size
        self._update_model_file(self._model_file.id, size=size)

    def _update_model_file_progress(self, model_file_id: int, progress: float):
        self._update_model_file(model_file_id, download_progress=progress)

    def _update_model_file(self, id: int, **kwargs):
        model_file_public = self._clientset.model_files.get(id=id)

        model_file_update = ModelFileUpdate(**model_file_public.model_dump())
        for key, value in kwargs.items():
            setattr(model_file_update, key, value)

        self._clientset.model_files.update(id=id, model_update=model_file_update)
